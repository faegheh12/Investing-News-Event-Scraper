{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract data of a News from news sites that used in investing.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### www.fxstreet.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "## www.fxstreet.com\n",
    "\n",
    "def extract_fxstreet (url_add):\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url_add)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the specific <div> you want to extract content from (change 'id' to the appropriate attribute)\n",
    "    target_div = soup.find('div', {'id': 'fxs_article_content'})\n",
    "    header_news = soup.find('header', class_='fxs_article_header').find('h1').get_text()\n",
    "    date_news = soup.find('time').get_text()\n",
    "\n",
    "    # Initialize an empty list to store the extracted text in order\n",
    "    all_text = []\n",
    "    target_tags = ['p', 'li', 'h1', 'h2']\n",
    "\n",
    "    for element in target_div.descendants:\n",
    "        if element.name in target_tags:\n",
    "            text = element.get_text().strip()\n",
    "            if text:  # Check if the text is not empty\n",
    "                all_text.append(text)\n",
    "\n",
    "    final_text = '\\n'.join(all_text)\n",
    "\n",
    "    # Print or save the final_text\n",
    "    return final_text, header_news, date_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### investing.com (new version of news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "## investing.com\n",
    "\n",
    "def extract_investing (url_add):\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url_add)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the specific <div> you want to extract content from (change 'id' to the appropriate attribute)\n",
    "    target_section = soup.find('div', {'class': 'WYSIWYG articlePage'})\n",
    "    header_news = soup.find('h1', class_='articleHeader').get_text()\n",
    "    div_date = soup.findAll('div', class_='contentSectionDetails')[1]\n",
    "    span = div_date.find('span')\n",
    "    date_news = span.get_text()\n",
    "\n",
    "    \n",
    "    ignore_div = target_section.findAll('div', class_='relatedInstrumentsWrapper')\n",
    "    for div in ignore_div:\n",
    "        div.extract()\n",
    "\n",
    "    # Initialize an empty list to store the extracted text in order\n",
    "    all_text = []\n",
    "    target_tags = ['p', 'li', 'h1', 'h2', 'span']\n",
    "\n",
    "    for element in target_section.descendants:\n",
    "        if element.name in target_tags:\n",
    "            text = element.get_text().strip()\n",
    "            if text:  # Check if the text is not empty\n",
    "                all_text.append(text)\n",
    "\n",
    "    final_text = '\\n'.join(all_text)\n",
    "\n",
    "    # Print or save the final_text\n",
    "    return final_text, header_news, date_news[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### investing.com (old version of news_ 2014 >>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "## investing.com\n",
    "\n",
    "def extract_investing_1 (url_add):\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url_add)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the specific <div> you want to extract content from (change 'id' to the appropriate attribute)\n",
    "    target_section = soup.find('div', {'class': 'WYSIWYG articlePage'})\n",
    "    header_news = soup.find('h1', class_='articleHeader').get_text()\n",
    "    div_date = soup.findAll('div', class_='contentSectionDetails')[1]\n",
    "    span = div_date.find('span')\n",
    "    date_news = span.get_text()\n",
    "\n",
    "    \n",
    "    ignore_div = target_section.findAll('div')\n",
    "    for div in ignore_div:\n",
    "        div.extract()\n",
    "\n",
    "    final_text = target_section.get_text(strip=True)\n",
    "\n",
    "    # Print or save the final_text\n",
    "    return final_text, header_news, date_news[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### www.forexlive.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an HTTP GET request to the URL\n",
    "\n",
    "## www.forexlive.com\n",
    "\n",
    "def extract_forexlive (url_add):\n",
    "\n",
    "    headers = {\n",
    "        'authority': 'www.forexlive.com',\n",
    "        'accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',\n",
    "        'accept-language': 'en-US,en;q=0.9',\n",
    "        'cookie': 'visid_incap_1058382=NRNNK65BQs2vwillZpJvYu+XP2UAAAAAQUIPAAAAAABZ+3bSltS5gkE7kVRhPBly; incap_ses_536_1058382=QIV3SZlV/R3XlNQVN0JwB++XP2UAAAAAyAbGQBXRbghPrRf5lLj10w==; OptanonConsent=isGpcEnabled=0&datestamp=Mon+Oct+30+2023+15%3A18%3A02+GMT%2B0330+(Iran+Standard+Time)&version=202305.1.0&browserGpcFlag=0&isIABGlobal=false&hosts=&landingPath=https%3A%2F%2Fwww.forexlive.com%2FEducation%2Fecb-rate-pause-impact-on-eurusd-20231030%2F&groups=C0005%3A1%2CC0003%3A1%2C;... (your cookie here)',\n",
    "        'referer': 'f{url_add}',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"118\", \"Google Chrome\";v=\"118\", \"Not=A?Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-mobile': '?1',\n",
    "        'sec-ch-ua-platform': 'Android',\n",
    "        'sec-fetch-dest': 'empty',\n",
    "        'sec-fetch-mode': 'no-cors',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Mobile Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url_add, headers=headers)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the specific <div> you want to extract content from (change 'id' to the appropriate attribute)\n",
    "    target_section = soup.find('article', {'class': 'article__content-body article-body'})\n",
    "    header_news = soup.find('h1', class_='article__title').get_text()\n",
    "    date_news = soup.find('div', class_='publisher-details__date date hide-on-mobile').get_text()\n",
    "\n",
    "    # Initialize an empty list to store the extracted text in order\n",
    "    all_text = []\n",
    "    target_tags = ['p', 'li', 'h1', 'h2']\n",
    "\n",
    "    for element in target_section.descendants:\n",
    "        if element.name in target_tags:\n",
    "            text = element.get_text().strip()\n",
    "            if text:  # Check if the text is not empty\n",
    "                all_text.append(text)\n",
    "\n",
    "\n",
    "    final_text = '\\n'.join(all_text)\n",
    "\n",
    "    # Print or save the final_text\n",
    "    return final_text, header_news, date_news[9:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### marketpulse.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "## marketpulse.com\n",
    "\n",
    "# Get the current date and time\n",
    "current_datetime = datetime.now()\n",
    "\n",
    "# Function to convert relative timestamp to absolute timestamp\n",
    "def convert_relative_to_absolute(relative_time):\n",
    "    # Extract the number of hours from the relative time\n",
    "    hours_ago = int(relative_time.split()[1])\n",
    "\n",
    "    # Calculate the absolute timestamp by subtracting the relative time\n",
    "    publish_time = current_datetime - relativedelta(hours=hours_ago)\n",
    "    return publish_time.strftime(\"%b %d, %Y %H:%M\")\n",
    "\n",
    "\n",
    "def extract_marketpulse (url_add):\n",
    "\n",
    "    response = requests.get(url_add)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the specific <div> you want to extract content from (change 'id' to the appropriate attribute)\n",
    "    target_section = soup.find('div', {'class': 'content-replace'})\n",
    "    header_news = soup.find('h1', class_='entry-title').get_text()\n",
    "    publish_date = soup.find('time', class_='time wib post-date updated').get_text()\n",
    "\n",
    "    if publish_date.endswith(\"ago\"):\n",
    "        # Convert the relative timestamp to an absolute timestamp\n",
    "        publish_time = convert_relative_to_absolute(publish_date)\n",
    "    else:\n",
    "        # Parse the absolute timestamp\n",
    "        publish_time = publish_date\n",
    "\n",
    "\n",
    "    # Initialize an empty list to store the extracted text in order\n",
    "    all_text = []\n",
    "    target_tags = ['p', 'li', 'h1', 'h2']\n",
    "\n",
    "    for element in target_section.descendants:\n",
    "        if element.name in target_tags:\n",
    "            text = element.get_text().strip()\n",
    "            if text:  # Check if the text is not empty\n",
    "                all_text.append(text)\n",
    "\n",
    "    final_text = '\\n'.join(all_text)\n",
    "\n",
    "    # Print the final_text\n",
    "    return final_text, header_news, publish_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### futurecurrencyforecast.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "## futurecurrencyforecast.com\n",
    "\n",
    "def extract_futurecurrencyforecast(url_add):\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url_add)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the specific <div> you want to extract content from (change 'id' to the appropriate attribute)\n",
    "    target_section = soup.find('section', {'class': 'entry-content'})\n",
    "    header_news = soup.find('h1', class_='entry-title').get_text()\n",
    "    date_news = soup.find('time', class_='updated').get_text()\n",
    "\n",
    "\n",
    "    # Initialize an empty list to store the extracted text in order\n",
    "    all_text = []\n",
    "    target_tags = ['p', 'li', 'h1', 'h2']\n",
    "\n",
    "    for element in target_section.descendants:\n",
    "        if element.name in target_tags:\n",
    "            text = element.get_text().strip()\n",
    "            if text:  # Check if the text is not empty\n",
    "                all_text.append(text)\n",
    "\n",
    "    final_text = '\\n'.join(all_text)\n",
    "\n",
    "    # Print or save the final_text\n",
    "    return final_text, header_news, date_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### euroexchangeratenews.co.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "## euroexchangeratenews.co.uk\n",
    "\n",
    "def extract_euroexchangeratenews(url_add):\n",
    "\n",
    "    # Send an HTTP GET request to the URL\n",
    "    response = requests.get(url_add)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the specific <div> you want to extract content from (change 'id' to the appropriate attribute)\n",
    "    target_section = soup.find('section', {'class': 'entry-content'})\n",
    "    header_news = soup.find('h1', class_='entry-title').get_text()\n",
    "    date_news = soup.find('time', class_='updated').get_text()\n",
    "\n",
    "\n",
    "    # Initialize an empty list to store the extracted text in order\n",
    "    all_text = []\n",
    "    target_tags = ['p', 'li', 'h1', 'h2']\n",
    "\n",
    "    for element in target_section.descendants:\n",
    "        if element.name in target_tags:\n",
    "            text = element.get_text().strip()\n",
    "            if text:  # Check if the text is not empty\n",
    "                all_text.append(text)\n",
    "\n",
    "    final_text = '\\n'.join(all_text)\n",
    "\n",
    "    # Print or save the final_text\n",
    "    return final_text, header_news, date_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all News from www.dailyfx.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Name Site Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def site_detection(url):\n",
    "\n",
    "    if url.startswith('https://www.fxstreet.com'):\n",
    "        site = 'fxstreet'\n",
    "    elif url.startswith('https://uk.investing.com'):\n",
    "        site = 'investing'\n",
    "    elif url.startswith('https://www.forexlive.com'):\n",
    "        site = 'forexlive'\n",
    "    elif url.startswith('https://www.marketpulse.com'):\n",
    "        site = 'marketpulse'\n",
    "    elif url.startswith('https://www.futurecurrencyforecast.com/'):\n",
    "        site = 'futurecurrencyforecast'\n",
    "    elif url.startswith('https://www.euroexchangeratenews.co.uk/'):\n",
    "        site = 'euroexchangeratenews'\n",
    "\n",
    "    return site "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### crawl news of investing.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "https://uk.investing.com/news/forex-news/pound-to-euro-week-ahead-forecast-potential-downside-after-a-tear-higher-3256943\n",
      "https://uk.investing.com/news/forex-news/bitcoin-hits-40000-level-for-the-first-time-this-year-3256838\n",
      "https://uk.investing.com/news/forex-news/asia-fx-muted-dollar-steadies-with-fed-rate-cuts-in-focus-3256916\n",
      "https://www.fxstreet.com/news/gbp-usd-further-gains-likely-above-12745-uob-202312040709\n",
      "https://www.fxstreet.com/news/eur-usd-downward-bias-gathers-pace-uob-202312040621\n",
      "https://www.fxstreet.com/news/gbp-usd-remains-capped-below-12700-us-services-pmi-eyed-202312040356\n",
      "https://www.fxstreet.com/news/gold-price-pulls-back-after-rising-to-fresh-all-time-high-bullish-potential-remains-intact-202312040349\n",
      "https://www.fxstreet.com/news/usd-inr-loses-ground-eyes-on-indian-services-pmi-rbi-rate-decision-202312040312\n",
      "https://www.fxstreet.com/analysis/gold-price-forecast-xau-usd-hits-fresh-all-time-highs-near-2-150-whats-next-202312040306\n",
      "https://www.fxstreet.com/news/japanese-yen-advances-to-near-three-month-high-against-usd-seems-poised-to-appreciate-further-202312040146\n",
      "https://www.fxstreet.com/news/nzd-usd-extends-its-upside-above-06200-on-the-softer-usd-fed-rate-cut-bet-202312040118\n",
      "https://www.fxstreet.com/news/pboc-sets-usd-cny-reference-rate-at-71011-vs-71104-previous-202312040117\n",
      "https://www.fxstreet.com/news/eur-usd-holds-below-10900-focus-on-german-trade-balance-ecbs-lagarde-speech-202312040033\n",
      "https://www.forexlive.com/centralbank/pboc-is-expected-to-set-the-usdcny-reference-rate-at-71271-reuters-model-20231204/\n",
      "https://www.fxstreet.com/news/aud-usd-posts-modest-gain-above-06670-rba-rate-decision-us-ism-services-pmi-eyed-202312032301\n",
      "https://uk.investing.com/news/forex-news/britain-yet-to-make-the-case-for-a-digital-pound-lawmakers-say-3256557\n",
      "https://www.fxstreet.com/news/eur-gbp-dives-to-september-lows-and-indicators-flash-oversold-conditions-202312012142\n",
      "https://www.fxstreet.com/news/eur-chf-chalks-in-a-third-straight-day-of-losses-as-euro-declines-into-09450-against-the-swiss-franc-202312012107\n",
      "https://www.fxstreet.com/news/eur-usd-clears-daily-losses-and-defends-the-20-day-sma-closes-a-losing-week-202312012001\n",
      "https://www.fxstreet.com/news/nzd-usd-re-taps-06200-for-the-second-time-in-a-week-as-hawkish-rbnz-bolsters-kiwi-202312011931\n",
      "https://www.fxstreet.com/news/gbp-usd-rallies-above-12700-amid-speculations-of-feds-easing-cycle-soft-us-dollar-202312011907\n",
      "https://www.fxstreet.com/news/eur-gbp-sees-ten-week-lows-near-08580-as-euro-extends-declines-agaisnt-the-pound-sterling-202312011713\n",
      "https://uk.investing.com/news/forex-news/ukraines-central-bank-to-remove-foreign-currency-cash-sale-limits--governor-3255835\n",
      "https://www.fxstreet.com/news/gold-price-forecast-xau-usd-surges-to-a-seven-month-high-amid-powells-comments-202312011659\n",
      "https://uk.investing.com/news/forex-news/pounddollar-looks-for-a-santa-upswing-3255760\n",
      "https://www.fxstreet.com/news/usd-jpy-dips-below-ichimoku-cloud-on-weak-us-pmi-powell-speech-202312011622\n",
      "https://www.fxstreet.com/news/canadian-dollar-pushes-higher-after-canada-adds-more-jobs-than-expected-202312011623\n",
      "https://www.fxstreet.com/news/aud-usd-ascends-after-us-ism-pmis-eyes-on-powells-speech-202312011619\n",
      "https://www.fxstreet.com/analysis/gold-price-weekly-forecast-xau-usd-looking-at-record-highs-and-us-jobs-data-202312011606\n",
      "https://www.fxstreet.com/news/gold-price-forecast-xau-usd-to-settle-above-2-100-in-2024-tds-202312011559\n",
      "https://www.fxstreet.com/news/mexican-peso-appreciates-against-the-us-dollar-ahead-of-powells-speech-202312011534\n",
      "https://www.fxstreet.com/analysis/gbp-usd-weekly-forecast-pound-sterling-remains-buy-the-dip-trade-202312011528\n",
      "https://www.forexlive.com/news/usdjpy-slumps-after-ism-manufacturing-and-goolsbee-20231201/\n",
      "https://www.fxstreet.com/analysis/eur-usd-weekly-forecast-inflation-eases-could-employment-data-finally-fit-central-banks-wishes-202312011519\n",
      "https://www.fxstreet.com/news/eur-usd-the-return-below-10915-could-herald-a-deeper-pullback-socgen-202312011516\n",
      "2\n",
      "https://uk.investing.com/news/forex-news/dollar-retreats-after-pce-data-points-to-cooling-inflation-powell-set-to-speak-3255603\n",
      "https://uk.investing.com/news/forex-news/house-price-rise-underscores-pounds-weekly-gain-against-euro-3255511\n",
      "https://uk.investing.com/news/forex-news/asia-fx-muted-dollar-rebounds-before-powell-speech-3255335\n",
      "https://uk.investing.com/news/forex-news/pound-rises-as-bank-of-englands-message-hits-home-says-bofa-3254145\n",
      "https://uk.investing.com/news/forex-news/dollar-recovers-from-threemonth-lows-pce-data-looms-large-3254120\n",
      "https://uk.investing.com/news/forex-news/pound-to-dollar-december-outlook-3253908\n",
      "https://uk.investing.com/news/forex-news/pounddollar-overbought-on-breach-of-127-3252697\n",
      "https://uk.investing.com/news/forex-news/dollar-stabilizes-at-low-levels-pce-data-could-determine-future-direction-3252489\n",
      "https://uk.investing.com/news/forex-news/pound-sterling-up-against-euro--dollar-as-banks-message-hits-home-3252268\n",
      "https://uk.investing.com/news/forex-news/asia-fx-rises-on-fed-pivot-hopes-nz-dollar-boosted-by-hawkish-rbnz-3252209\n",
      "https://uk.investing.com/news/forex-news/dollaryen-pullback-extends-xmcom-3251163\n",
      "https://uk.investing.com/news/forex-news/dollar-edges-higher-but-near-monthly-lows-ahead-of-pce-inflation-3251066\n",
      "https://uk.investing.com/news/forex-news/pound-sterling-hits-new-peaks-against-euro-dollar-on-receding-rate-cut-bets-3250887\n",
      "https://uk.investing.com/news/forex-news/asia-fx-rises-as-easing-fed-fears-put-dollar-at-3month-low-3250790\n",
      "https://uk.investing.com/news/forex-news/dollar-slips-ahead-of-key-inflation-data-sterling-shows-strength-3249583\n",
      "https://uk.investing.com/news/forex-news/pound-to-euro-week-ahead-forecast-flipping-positive-eurozone-cpi-in-focus-3249464\n",
      "https://uk.investing.com/news/forex-news/asia-fx-inches-lower-dollar-steady-before-swarm-of-economic-data-3249378\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Initialize an empty list to store dictionaries containing news data\n",
    "news_data = []\n",
    "\n",
    "# page count of news site\n",
    "num = 3\n",
    "\n",
    "for i in range(1, num):\n",
    "    print(i)\n",
    "\n",
    "    # Send an HTTP GET request to the URL of the webpage\n",
    "    first_url = f'https://uk.investing.com/news/forex-news/{i}'  # Replace with the actual URL\n",
    "    response = requests.get(first_url)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the section with id \"leftColumn\"\n",
    "    left_column = soup.find('section', {'id': 'leftColumn'})\n",
    "\n",
    "    # Find all the div elements with class \"textDiv\" within the \"leftColumn\" section\n",
    "    text_divs = left_column.find_all('div', {'class': 'textDiv'})\n",
    "\n",
    "    # Extract and attributes from any news\n",
    "    for div in text_divs:\n",
    "        a_element = div.find('a')\n",
    "        if a_element:\n",
    "            href = a_element.get('href')\n",
    "            if href.startswith('/news'):\n",
    "                href = f'https://uk.investing.com{href}'\n",
    "            elif href.startswith('https://invst.ly/'):\n",
    "                # Send an HTTP HEAD request to get the final URL\n",
    "                response_1 = requests.head(href, allow_redirects=True)\n",
    "\n",
    "                # The final URL is in the response's URL\n",
    "                href = response_1.url\n",
    "            print(href)\n",
    "            ref_site = site_detection(href)\n",
    "            extract_function = getattr(sys.modules[__name__], f'extract_{ref_site}')\n",
    "            fi_text, he_news, date = extract_function(href)\n",
    "            \n",
    "\n",
    "            news_data.append({\n",
    "                'link': href,\n",
    "                'date': date,\n",
    "                'title': he_news,\n",
    "                'news_content': fi_text\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "News = pd.DataFrame(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "News"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
